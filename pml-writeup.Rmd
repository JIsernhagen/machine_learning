---
title: "PML-Assignment-Writeup"
author: "Jonathan Isernhagen"
date: "December 26, 2015"
output: html_document
---

###Synopsis
We have been asked to analyze a data set of metrics related to the performance of weight-lifting physical exercises, with each corresponding to:
A) a properly-performed exercise, or;
B-E) an exercise improperly-performed in a specific way.
Our task is to use any and all useful metrics to ascertain which category "bucket" of correct or incorrect exercise each observation belongs to, and predict the A-E category for a set of twenty new observations with the highest possible degree of accuracy.  Our preference in this context is for accuracy over interpretability or scalability.

###Executive Summary
After deleting all variables which contained few or no values in order to concentrate the algorithms' efforts on useful data of manageable size, and creating a simple random forest model with no preprocessing of the data and a simple 60/40 split of the data into testing/training (with no cross-validation, boosting or bootstrapping) we discovered a model which gave us very high accuracy on both the training and testing data sets and perfect (100%) accuracy on the validation set.

###Analyses 
**Exploratory Analysis:** 

We first installed the caret package, ggplot2 for charting and dplyr for data manipulation, then imported the "training" (which we labeled as training.and.testing) and "testing" (which we labeled "validation") data sets.
```{R message=FALSE, error=FALSE, warning=FALSE}
require(caret); require(randomForest); require(ggplot2); require(dplyr)
training.and.testing<-read.csv("pml-training.csv")
validation<-read.csv("pml-testing.csv")
```

....and used str to observe their characteristics (Appendix A).  The training.and.testing set includes 19,622 observations of 160 variables, qualifying it as a "large" data set.  It was apparent that many columns of both data sets contained data for only a small minority of observations.  Attempting to fit a random forest model against the un-edited data set took > 1 hour, so we decided to delete all non-exercise-performance-related (e.g. timestamp) and all sparsely-populated columns, and then make other changes necessary to align the two data sets:
```{r}
training.and.testing.cut<-select(training.and.testing, new_window:total_accel_belt, gyros_belt_x:total_accel_arm, gyros_arm_x:magnet_arm_z, roll_dumbbell:yaw_dumbbell, total_accel_dumbbell, gyros_dumbbell_x:yaw_forearm, total_accel_forearm, gyros_forearm_x:magnet_forearm_z, classe) 
validation.cut<-select(validation, new_window:total_accel_belt, gyros_belt_x:total_accel_arm, gyros_arm_x:magnet_arm_z, roll_dumbbell:yaw_dumbbell, total_accel_dumbbell, gyros_dumbbell_x:yaw_forearm, total_accel_forearm, gyros_forearm_x:magnet_forearm_z)
validation.cut$magnet_forearm_y<-as.numeric(validation.cut$magnet_forearm_y)
validation.cut$magnet_forearm_z<-as.numeric(validation.cut$magnet_forearm_z)
levels(validation.cut$new_window) <- c("no", "yes")
```

We then divided the "training.and.testing" data set into "training" and "testing" partitions.
```{r}
inTrain<-createDataPartition(y=training.and.testing.cut$classe, p = 0.6, list = FALSE)
training<-training.and.testing.cut[inTrain,]
testing<-training.and.testing.cut[-inTrain,]
```


**Model #1:  random forest, no preprocessing, 60/40 split:** Our first model was a simple random forest.  Evoking the random forest from within caret (modelFit<-train(classe ~ ., data = training, method="rf")) took too long, so we did it directly.
```{r}
set.seed(1)
modelFit<-randomForest(classe ~ ., data = training)
modelFit
```

**Diagnostics:** 
The model's error OOB error rate is 0.35%, which is much lower than we expected.  When applied to the testing data set, the confusion matrix appears as follows:

```{r}
predictions<-predict(modelFit, newdata=testing)
confusionMatrix(predictions, testing$classe)
```
...which showed an encouragingly very low error rate (we expected the testing error rate to be significantly higher than the training error rate because of overfitting), and gave us the confidence to proceed directly to the validation phase and use one of our two submission "bullets" to see if the model was in fact highly predictive.

The predictions achieved when applying the model to the validation set were as follows:
```{r}
predictions<-predict(modelFit, newdata=validation.cut)
predictions
```

...which turned out to be 100% correct. At this point we decided not to construct other models, but rather to try to figure out what is happening within this model that makes it so effective.  We used varImp to ascertain which variables were most important to the model we'd built (Appendix B), and then charted the two most important variables against each other while coloring by classe

```{R fig.height = 5, fig.width = 10}
qplot(num_window,roll_belt, colour = classe, data=training)
```

It is readily apparent why these two predictors are so valuable:  the classe categories stand apart from each other in sharp relief when they are charted against one another, much more sharply than in any of the charts used in the lecture examples in class.

###Conclusions
We came to the exercise prepared to test a wide variety of models and methods to achieve high predictive accuracy, but the very first model we tried, with minimal, common-sense shaping of the data, provided a model sufficiently accurate for us to score 100% of the validation observations successfully, so we stopped the exercise and are submitting these findings.

###Appendices:
**Appendix A:  str(Training) and str(Validation)**
```{r}
str(training)
str(testing)
```

**Appendix B:  Most important variables** 
```{r}
varImp(modelFit)
```
